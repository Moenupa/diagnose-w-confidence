"""
vLLMæœåŠ¡ - æ”¯æŒOpenAI APIå’ŒåŸå§‹logitsæå–
æ”¯æŒå¤šæ¨¡æ€æ¨¡å‹å’Œç½®ä¿¡åº¦è®¡ç®—
"""
import os
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from vllm import LLM, SamplingParams
import uvicorn
import torch
from typing import List, Optional, Dict, Any
import numpy as np
from PIL import Image
import requests
from io import BytesIO

# å¯¼å…¥ç½®ä¿¡åº¦è®¡ç®—æ¨¡å—
from confidence import calculate_confidence, aggregate_sentence_confidence

os.environ["VLLM_USE_V1"] = "0"

app = FastAPI(title="vLLM with Logits")

# é…ç½®
MODEL_NAME = os.getenv("MODEL_NAME", "Qwen/Qwen2.5-0.5B-Instruct")
HOST = os.getenv("HOST", "0.0.0.0")
PORT = int(os.getenv("PORT", "8000"))
TENSOR_PARALLEL_SIZE = int(os.getenv("TENSOR_PARALLEL_SIZE", "1"))
MAX_MODEL_LEN = int(os.getenv("MAX_MODEL_LEN", "1024"))
GPU_MEMORY_UTIL = float(os.getenv("GPU_MEMORY_UTIL", "0.65"))
ENFORCE_EAGER = os.getenv("ENFORCE_EAGER", "True").lower() == "true"

llm = None

def download_image(url: str) -> Image.Image:
    """ä»URLä¸‹è½½å›¾åƒå¹¶è¿”å›PIL Imageå¯¹è±¡"""
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        return Image.open(BytesIO(response.content))
    except Exception as e:
        raise HTTPException(400, f"Failed to download image from {url}: {str(e)}")

class LogitsSpy:
    def __init__(self):
        self.processed_logits: List[torch.Tensor] = []
    
    def __call__(self, token_ids: List[int], logits: torch.Tensor) -> torch.Tensor:
        # è½¬æ¢ä¸ºfloat32ä»¥é¿å…BFloat16å…¼å®¹æ€§é—®é¢˜
        self.processed_logits.append(logits.detach().cpu().float())
        return logits

class ChatMessage(BaseModel):
    role: str
    content: str | List[Dict[str, Any]]  # æ”¯æŒå¤šæ¨¡æ€å†…å®¹

class ChatRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    max_tokens: int = 100
    temperature: float = 0.7
    images: Optional[List[str]] = None  # å¯é€‰çš„å›¾åƒåˆ—è¡¨

class CompletionRequest(BaseModel):
    model: str
    prompt: str
    max_tokens: int = 100
    temperature: float = 0.7

class LogitsRequest(BaseModel):
    prompt: str | Dict[str, Any]  # æ”¯æŒæ–‡æœ¬æˆ–å¤šæ¨¡æ€è¾“å…¥
    max_tokens: int = 100
    temperature: float = 0.7
    top_p: float = 0.95
    top_k_logits: int = Field(default=10, description="è¿”å›top-kï¼Œ-1è¿”å›å…¨éƒ¨")
    return_full_logits: bool = Field(default=False, description="è¿”å›å®Œæ•´logitså‘é‡")
    calculate_reliability: bool = Field(default=True, description="è®¡ç®—æ•´å¥Reliability")
    reliability_k_tokens: int = Field(default=5, description="è®¡ç®—Reliabilityæ—¶ä½¿ç”¨çš„top-k tokens")
    images: Optional[List[str]] = None  # å¯é€‰çš„å›¾åƒURLåˆ—è¡¨
    image_url: Optional[str] = None  # å…¼å®¹å•ä¸ªå›¾åƒURL

@app.on_event("startup")
async def startup():
    global llm
    print("=" * 80)
    print(f"ğŸš€ Loading {MODEL_NAME}")
    print(f"ğŸ“ {HOST}:{PORT}")
    
    # æ£€æµ‹æ˜¯å¦æ˜¯å¤šæ¨¡æ€æ¨¡å‹
    is_multimodal = "vl" in MODEL_NAME.lower() or "vision" in MODEL_NAME.lower()
    if is_multimodal:
        print("ğŸ–¼ï¸  Multimodal model detected")
    
    print("=" * 80)
    
    llm_kwargs = {
        "model": MODEL_NAME,
        "tensor_parallel_size": TENSOR_PARALLEL_SIZE,
        "max_model_len": MAX_MODEL_LEN,
        "gpu_memory_utilization": GPU_MEMORY_UTIL,
        "trust_remote_code": True,
        "enforce_eager": ENFORCE_EAGER,
    }
    
    # å¤šæ¨¡æ€æ¨¡å‹ç‰¹æ®Šé…ç½®
    if is_multimodal:
        llm_kwargs["limit_mm_per_prompt"] = {"image": 4}  # æ”¯æŒæœ€å¤š4å¼ å›¾ç‰‡
    print(llm_kwargs)
    llm = LLM(**llm_kwargs)
    print("âœ… Ready!")
    print(f"âœ… Confidence calculation enabled (LogTokU method)")
    if is_multimodal:
        print(f"âœ… Multimodal support enabled")

@app.post("/v1/chat/completions")
async def chat_completion(request: ChatRequest):
    if llm is None:
        raise HTTPException(503, "Model not loaded")
    
    prompt = "\n".join([f"{m.role}: {m.content}" for m in request.messages])
    prompt += "\nassistant:"
    
    outputs = llm.generate([prompt], SamplingParams(
        max_tokens=request.max_tokens,
        temperature=request.temperature
    ))
    
    return {
        "id": "chat-" + os.urandom(4).hex(),
        "object": "chat.completion",
        "model": request.model,
        "choices": [{
            "message": {"role": "assistant", "content": outputs[0].outputs[0].text},
            "finish_reason": "stop"
        }]
    }

@app.post("/v1/completions")
async def completion(request: CompletionRequest):
    if llm is None:
        raise HTTPException(503, "Model not loaded")
    
    outputs = llm.generate([request.prompt], SamplingParams(
        max_tokens=request.max_tokens,
        temperature=request.temperature
    ))
    
    return {
        "id": "cmpl-" + os.urandom(4).hex(),
        "object": "text_completion",
        "model": request.model,
        "choices": [{"text": outputs[0].outputs[0].text, "finish_reason": "stop"}]
    }

@app.post("/v1/completions_with_logits")
async def completion_with_logits(request: LogitsRequest):
    if llm is None:
        raise HTTPException(503, "Model not loaded")
    
    try:
        logits_spy = LogitsSpy()
        
        # å¤„ç†è¾“å…¥ (æ”¯æŒæ–‡æœ¬å’Œå¤šæ¨¡æ€)
        if isinstance(request.prompt, dict):
            # å¤šæ¨¡æ€è¾“å…¥
            prompt_input = request.prompt
        else:
            # çº¯æ–‡æœ¬è¾“å…¥
            text_prompt = request.prompt
            
            # å¦‚æœæä¾›äº†image_urlæˆ–images,æ„å»ºå¤šæ¨¡æ€è¾“å…¥
            if request.image_url or request.images:
                image_urls = []
                if request.image_url:
                    image_urls.append(request.image_url)
                if request.images:
                    image_urls.extend(request.images)
                
                # ä¸‹è½½å›¾åƒå¹¶è½¬æ¢ä¸ºPIL Imageå¯¹è±¡
                pil_images = [download_image(url) for url in image_urls]
                
                # ä¸ºQwen2-VLæ·»åŠ å›¾åƒå ä½ç¬¦
                # æ¯å¼ å›¾ç‰‡éœ€è¦ä¸€ä¸ª <|image_pad|> å ä½ç¬¦
                image_placeholders = "<|image_pad|>" * len(pil_images)
                prompt_with_placeholder = f"{image_placeholders}{text_prompt}"
                
                # æ„å»ºvLLMçš„å¤šæ¨¡æ€è¾“å…¥æ ¼å¼
                # å¯¹äºå•å¼ å›¾ç‰‡,ç›´æ¥ä¼ é€’Imageå¯¹è±¡;å¤šå¼ å›¾ç‰‡ä¼ é€’åˆ—è¡¨
                prompt_input = {
                    "prompt": prompt_with_placeholder,
                    "multi_modal_data": {
                        "image": pil_images[0] if len(pil_images) == 1 else pil_images
                    }
                }
            else:
                prompt_input = text_prompt
        
        outputs = llm.generate([prompt_input], SamplingParams(
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            top_p=request.top_p,
            logits_processors=[logits_spy]
        ))
        
        output = outputs[0].outputs[0]
        result_logits = []
        
        # ç”¨äºè®¡ç®— reliability çš„æ•°æ®
        eu_2_scores = []
        au_2_scores = []
        
        for step_idx, step_logits in enumerate(logits_spy.processed_logits):
            # å¤„ç†å½¢çŠ¶
            if len(step_logits.shape) == 2:
                logits_tensor = step_logits[0]
            elif len(step_logits.shape) == 3:
                logits_tensor = step_logits[0, 0]
            else:
                logits_tensor = step_logits
            
            vocab_size = logits_tensor.shape[0]
            
            # è®¡ç®—ç½®ä¿¡åº¦æŒ‡æ ‡ (ä½¿ç”¨ top-2 logits)
            if request.calculate_reliability:
                logits_np = logits_tensor.cpu().numpy()
                eu_2 = calculate_confidence(logits_np, mode="eu_2")
                au_2 = calculate_confidence(logits_np, mode="au_2")
                eu_2_scores.append(float(eu_2))
                au_2_scores.append(float(au_2))
            
            if request.return_full_logits:
                step_data = {
                    "step": step_idx,
                    "full_logits": logits_tensor.tolist(),
                    "vocab_size": vocab_size
                }
            else:
                k = vocab_size if request.top_k_logits == -1 else min(request.top_k_logits, vocab_size)
                top_values, top_indices = torch.topk(logits_tensor, k)
                
                step_data = {
                    "step": step_idx,
                    "top_k": [
                        {"token_id": int(idx), "logit": float(val), "rank": i + 1}
                        for i, (val, idx) in enumerate(zip(top_values, top_indices))
                    ],
                    "vocab_size": vocab_size
                }
            
            # æ·»åŠ ç½®ä¿¡åº¦ä¿¡æ¯åˆ°æ¯ä¸ªstep
            if request.calculate_reliability:
                step_data["eu_2"] = eu_2_scores[-1]
                step_data["au_2"] = au_2_scores[-1]
                step_data["uncertainty"] = eu_2_scores[-1] * au_2_scores[-1]  # EUÃ—AU
            
            result_logits.append(step_data)
        
        # è®¡ç®—æ•´å¥çš„ Reliability
        response = {
            "text": output.text,
            "token_ids": output.token_ids,
            "logits": result_logits,
            "num_steps": len(result_logits)
        }
        
        if request.calculate_reliability and len(eu_2_scores) > 0:
            combined = np.array(eu_2_scores) * np.array(au_2_scores)
            
            # æ•´å¥å¯é æ€§æŒ‡æ ‡
            avg_reliability = float(np.mean(combined))
            
            # Top-K æœ€ä¸ç¡®å®šçš„tokens (è®ºæ–‡æ–¹æ³•)
            k_tokens = min(request.reliability_k_tokens, len(combined))
            if k_tokens == len(combined):
                topk_reliability = avg_reliability
            else:
                top_k_indices = np.argpartition(combined, -k_tokens)[-k_tokens:]
                topk_reliability = float(np.mean(combined[top_k_indices]))
            
            response["reliability"] = {
                "method": "LogTokU (EUÃ—AU based on top-2 logits)",
                "avg_all_tokens": avg_reliability,
                "avg_uncertainty": avg_reliability,  # åˆ«å
                f"top_{k_tokens}_uncertain_tokens": topk_reliability,
                "sentence_reliability": -topk_reliability,  # è®ºæ–‡å®šä¹‰: Reliability = -AUÃ—EU
                "interpretation": {
                    "avg_uncertainty": "å¹³å‡ä¸ç¡®å®šæ€§ (æ‰€æœ‰tokensçš„EUÃ—AUå¹³å‡å€¼)",
                    "top_k_uncertainty": f"æœ€ä¸ç¡®å®šçš„{k_tokens}ä¸ªtokensçš„EUÃ—AUå¹³å‡å€¼",
                    "sentence_reliability": "æ•´å¥å¯é æ€§ (è¶Šæ¥è¿‘0è¶Šå¯é , è¶Šè´Ÿè¶Šä¸å¯é )"
                },
                "token_level": {
                    "eu_2": eu_2_scores,
                    "au_2": au_2_scores,
                    "uncertainty": combined.tolist()
                }
            }
        
        return response
    
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(500, f"Error: {str(e)}")

@app.get("/v1/models")
async def list_models():
    return {"object": "list", "data": [{"id": MODEL_NAME, "object": "model"}]}

@app.get("/health")
async def health():
    return {"status": "healthy" if llm else "not_ready", "model": MODEL_NAME}

if __name__ == "__main__":
    uvicorn.run(app, host=HOST, port=PORT)